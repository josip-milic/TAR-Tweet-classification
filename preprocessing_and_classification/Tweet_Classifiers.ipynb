{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of Zagreb<br>\n",
    "Faculty of Electrical Engineering and Computing\n",
    "\n",
    "# Text Analysis and Retrieval (TAR)\n",
    "\n",
    "<a href=\"http://www.fer.unizg.hr/predmet/apt\">http://www.fer.unizg.hr/predmet/apt</a>\n",
    "\n",
    "2015./2016.\n",
    "\n",
    "# Project theme 13: Tweet classification\n",
    "\n",
    "\n",
    "(c) 2016 Group nedovrs: Tomislav Marinković, Josip Milić, Domagoj Pereglin\n",
    "\n",
    "*Version 0.95*\n",
    "\n",
    "Date: **05.06.2016.**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tweet classifiers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Packages:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, time, string, pandas, pickle\n",
    "import numpy as np\n",
    "\n",
    "from subprocess import call\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "presentation_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Used directories:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annotations_dir  = '../annotations/'\n",
    "preprocessed_dir = 'processed/'\n",
    "intermediate_dir = 'intermediate/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tweet classes (categories):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_categories = {}\n",
    "tweet_categories[2] = 'DEALS'\n",
    "tweet_categories[3] = 'NEWS_TECHNOLOGY'\n",
    "tweet_categories[4] = 'NEWS_POLITICS'\n",
    "tweet_categories[5] = 'NEWS_SPORT'\n",
    "tweet_categories[6] = 'NEWS_REST'\n",
    "tweet_categories[7] = 'REST'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Annotations information loading:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated datasets: bugonline, hrtsport, indexhr, oglasnik, politikaplus, posaohr\n",
      "\n",
      "              Name  Count\n",
      "2            DEALS    976\n",
      "3  NEWS_TECHNOLOGY   1043\n",
      "4    NEWS_POLITICS   1657\n",
      "5       NEWS_SPORT   1052\n",
      "6        NEWS_REST   2738\n",
      "7             REST   3257\n",
      "\n",
      "Sum: 10723\n",
      "Number of categories: 6\n",
      "Most common category: 7 - REST\n"
     ]
    }
   ],
   "source": [
    "annotations_counter = {}\n",
    "annotations_file_names = os.listdir(annotations_dir)\n",
    "dataset_names = [x.split('_')[1] for x in annotations_file_names]\n",
    "print 'Annotated datasets: %s\\n' %  ', '.join(dataset_names)\n",
    "annotations = []\n",
    "for annotations_file_name in annotations_file_names:\n",
    "    annotation_file_lines = open(annotations_dir+annotations_file_name,'r').readlines()\n",
    "    annotations += [int(x.rstrip().split(';')[-1]) for x in annotation_file_lines]\n",
    "annotations_set = set(annotations)\n",
    "for c in annotations_set:\n",
    "    annotations_counter[c] = len(filter(lambda x : x==c,annotations))\n",
    "\n",
    "annotation_classes = sorted(annotations_counter.keys())\n",
    "header = ['Name','Count']\n",
    "print pandas.DataFrame([(tweet_categories[c],annotations_counter[c]) for c in annotation_classes], annotation_classes, header)   \n",
    "\n",
    "c_max,c_max_count = sorted(annotations_counter.items(),key=lambda x: x[1])[-1]\n",
    "print '\\nSum: %d' % sum(annotations_counter.values())\n",
    "print 'Number of categories: %d' % len(annotation_classes)\n",
    "print 'Most common category: %d - %s' % (c_max,tweet_categories[c_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Annotated preprocessed dataset loading:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset size: 10723\n"
     ]
    }
   ],
   "source": [
    "preprocessed_file_names = os.listdir(preprocessed_dir)\n",
    "preprocessed = []\n",
    "for preprocessed_file_name in preprocessed_file_names:\n",
    "    preprocessed += [x.rstrip().split('<|>') for x in open(preprocessed_dir+preprocessed_file_name,'r')]\n",
    "preprocessed = map(lambda (s,c): (s.rstrip(),int(c)), preprocessed)\n",
    "\n",
    "preprocessed = sorted(preprocessed)\n",
    "print 'Preprocessed dataset size: %d' % len(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset split:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_part = 0.7\n",
    "x_all = []\n",
    "y_all = []\n",
    "for s,c in preprocessed:\n",
    "    x_all.append(s)\n",
    "    y_all.append(c)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all,y_all,test_size=1-train_part, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Special features (words):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "tweet_categories[1] = 'PRIVATE'\n",
    "tweet_categories[2] = 'DEALS'\n",
    "tweet_categories[3] = 'NEWS_TECHNOLOGY'\n",
    "tweet_categories[4] = 'NEWS_POLITICS'\n",
    "tweet_categories[5] = 'NEWS_SPORT'\n",
    "tweet_categories[6] = 'NEWS_REST'\n",
    "tweet_categories[7] = 'REST'\n",
    "'''\n",
    "\n",
    "enhance_sets = {}\n",
    "enhance_sets[1] = set() # (PRIVATE) not used \n",
    "enhance_sets[2] = set(['posao','prodaja','stan','automobil','oglasnikpopusti','php','java','css','html','dev','androiddev','mysql','linux','c','programer','javite','sysadmin','developere','natječaj','konobar'])\n",
    "enhance_sets[3] = set(['seum','uber', 'airbnb','google','amazon','apple','haker','digitalan','gfxbench','mwc','samsung','gaming','twitter'])\n",
    "enhance_sets[4] = set(['udruga','franak','uhljeb','komisija','đukanović','jokić','šustar','banka'])\n",
    "enhance_sets[5] = set(['doping','uefacom','ademi','moo','čačić','euro','nogomet','gnkdinamo'])\n",
    "enhance_sets[6] = set(['naoblaka','vrijeme','sunčan','sunčano','naoblak','lokalan','pljusak','grmljavina','pretežno','pretežan','toplo'])\n",
    "enhance_sets[7] = set(['poveznica'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creation of TF-IDF vector:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enhance_vectors(list_of_data, tfidf_vectors_of_data, print_progress = True):\n",
    "    enhanced_vectors = tfidf_vectors_of_data.copy()\n",
    "    enhanced_vectors._shape = (tfidf_vectors_of_data.shape[0],tfidf_vectors_of_data.shape[1]+6)\n",
    "    indptr = enhanced_vectors.indptr\n",
    "    data = enhanced_vectors.data\n",
    "    indices = enhanced_vectors.indices\n",
    "    \n",
    "    \n",
    "    for i in range(tfidf_vectors_of_data.shape[0]):\n",
    "        words = set(list_of_data[i].strip().split(' '))\n",
    "        for ann in range(2,8):\n",
    "            enhancement = len(enhance_sets[ann].intersection(words))\n",
    "            data = np.insert(data,indptr[i+1],enhancement)\n",
    "            indices = np.insert(indices,indptr[i+1],tfidf_vectors_of_data.shape[1]+(ann-2))\n",
    "            for j in range(i+1,indptr.shape[0]):\n",
    "                indptr[j] += 1\n",
    "        \n",
    "        if (i % 1000 == 0) and i != 0:\n",
    "            if print_progress: print 'Enhanced: %d/%d vectors' % (i,tfidf_vectors_of_data.shape[0])\n",
    "        \n",
    "    csr_mat = csr_matrix( (data,indices,indptr), shape=enhanced_vectors.shape )\n",
    "    return csr_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Vectorizer and train and test TF-IDF vectors creation and enhancement:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples (TF-IDF vectors): 7506\n",
      "Number of test samples (TF-IDF vectors): 3217\n",
      "Vocabulary size: 14822\n",
      "\n",
      "arijan ademi danas biti pred @ uefacom iznositi svoj obrana u veza navodan korištenje doping\n",
      "Example vector (only nonzero values, index = 492) before enhancement:\n",
      "Vector: [0.34161449859165066, 0.2903083802509154, 0.2582952219082173, 0.26959725666388407, 0.29525031583425176, 0.32660842094820464, 0.2512243552424637, 0.32660842094820464, 0.34161449859165066, 0.3077029989545338, 0.21393204220662432, 0.1765465999287678, 0.08758828385256447]\n",
      "\n",
      "Enhancing 7506 vectors...\n",
      "Enhanced: 1000/7506 vectors\n",
      "Enhanced: 2000/7506 vectors\n",
      "Enhanced: 3000/7506 vectors\n",
      "Enhanced: 4000/7506 vectors\n",
      "Enhanced: 5000/7506 vectors\n",
      "Enhanced: 6000/7506 vectors\n",
      "Enhanced: 7000/7506 vectors\n",
      "Enhancement of 7506 vectors is finished!\n",
      "\n",
      "Example vector (only nonzero values, index = 492) after enhancement:\n",
      "Vector: [0.34161449859165066, 0.2903083802509154, 0.2582952219082173, 0.26959725666388407, 0.29525031583425176, 0.32660842094820464, 0.2512243552424637, 0.32660842094820464, 0.34161449859165066, 0.3077029989545338, 0.21393204220662432, 0.1765465999287678, 0.08758828385256447, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0]\n",
      "\n",
      "Enhancing 3217 vectors...\n",
      "Enhanced: 1000/3217 vectors\n",
      "Enhanced: 2000/3217 vectors\n",
      "Enhanced: 3000/3217 vectors\n",
      "Enhancement of 3217 vectors is finished!\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,)\n",
    "X_train = vectorizer.fit_transform(x_train)\n",
    "X_test = vectorizer.transform(x_test)\n",
    "print 'Number of training samples (TF-IDF vectors): %d' % X_train.shape[0]\n",
    "print 'Number of test samples (TF-IDF vectors): %d' % X_test.shape[0]\n",
    "print 'Vocabulary size: %d' % X_train.shape[1]\n",
    "print ''\n",
    "\n",
    "\n",
    "i = 492\n",
    "print x_train[i]\n",
    "print 'Example vector (only nonzero values, index = %d) before enhancement:' %i\n",
    "print 'Vector:', X_train.data[X_train.indptr[i]:X_train.indptr[i+1]].tolist()\n",
    "print ''\n",
    "\n",
    "print 'Enhancing %d vectors...' % X_train.shape[0]\n",
    "if presentation_mode:\n",
    "    X_train = pickle.load( open( \"saved_objects/X_train.pkl\", \"rb\" ) )\n",
    "else:\n",
    "    X_train = enhance_vectors(x_train, X_train)\n",
    "    pickle.dump( X_train, open( \"saved_objects/X_train.pkl\", \"wb\" ) )\n",
    "print 'Enhancement of %d vectors is finished!' % X_train.shape[0]\n",
    "\n",
    "print ''\n",
    "print 'Example vector (only nonzero values, index = %d) after enhancement:' %i\n",
    "print 'Vector:', X_train.data[X_train.indptr[i]:X_train.indptr[i+1]].tolist()\n",
    "\n",
    "print ''\n",
    "print 'Enhancing %d vectors...' % X_test.shape[0]\n",
    "if presentation_mode:\n",
    "    X_test = pickle.load( open( \"saved_objects/X_test.pkl\", \"rb\" ) )\n",
    "else:\n",
    "    X_test = enhance_vectors(x_test, X_test)\n",
    "    pickle.dump( X_test, open( \"saved_objects/X_test.pkl\", \"wb\" ) )\n",
    "print 'Enhancement of %d vectors is finished!' % X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Vectorizer and TF-IDF vectors of all data creation and enhancement:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhancing 10723 vectors...\n",
      "Enhanced: 1000/10723 vectors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-af15f31755f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mX_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"saved_objects/X_all.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mX_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menhance_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"saved_objects/X_all.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Enhancement of %d vectors is finished!'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mX_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-d31c1b5e4068>\u001b[0m in \u001b[0;36menhance_vectors\u001b[1;34m(list_of_data, tfidf_vectors_of_data, print_progress)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtfidf_vectors_of_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mann\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0mindptr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectorizer_all = TfidfVectorizer(sublinear_tf=True, max_df=0.7,)\n",
    "X_all = vectorizer_all.fit_transform(x_all)\n",
    "\n",
    "print 'Enhancing %d vectors...' % X_all.shape[0]\n",
    "if presentation_mode:\n",
    "    X_all = pickle.load( open( \"saved_objects/X_all.pkl\", \"rb\" ) )\n",
    "else:\n",
    "    X_all = enhance_vectors(x_all, X_all)\n",
    "    pickle.dump( X_all, open( \"saved_objects/X_all.pkl\", \"wb\" ) )\n",
    "print 'Enhancement of %d vectors is finished!' % X_all.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classifiers:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SVM classifier creation with optimal C:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_svm_classifier(X_train, y_train):\n",
    "    print 'Searching for optimal SVM classifier...'\n",
    "    parameters = {'C': [2*float(x)/20 for x in list(range(1,20,1))]}\n",
    "    print 'Parameters search:', parameters\n",
    "    #Cs = np.logspace(-10, -1, 10)\n",
    "    clf_svm = LinearSVC()\n",
    "    clf_svm_grid = GridSearchCV(estimator=clf_svm, param_grid=parameters, n_jobs=1)\n",
    "    clf_svm_grid.fit(X_train,y_train)\n",
    "    clf_svm = clf_svm_grid.best_estimator_\n",
    "    return clf_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LogReg classifier creation with optimal C:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_logReg_classifier(X_train,y_train):\n",
    "    print 'Searching for optimal LogReg classifier...'\n",
    "    #clf_log_reg = LogisticRegression(C = 1)\n",
    "    parameters = {'C': [float(x)/4 for x in list(range(1,180,10))]}\n",
    "    print 'Parameters search:', parameters\n",
    "    clf_log_reg = LogisticRegression()\n",
    "    clf_log_reg_grid = GridSearchCV(estimator=clf_log_reg, param_grid=parameters,n_jobs=1)\n",
    "    clf_log_reg_grid.fit(X_train,y_train)\n",
    "    clf_log_reg = clf_log_reg_grid.best_estimator_\n",
    "    return clf_log_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>KNN classifier creation with optimal number of neighbors:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_knn_classifier(X_train,y_train):\n",
    "    print 'Searching for optimal KNN classifier...'\n",
    "    #clf_knn = KNeighborsClassifier(n_neighbors=40)\n",
    "    parameters = {'n_neighbors':list(range(1,50,3))}\n",
    "    print 'Parameters search:', parameters\n",
    "    clf_knn = KNeighborsClassifier()\n",
    "    clf_knn_reg_grid = GridSearchCV(estimator=clf_knn, param_grid=parameters, n_jobs=1)\n",
    "    clf_knn_reg_grid.fit(X_train,y_train)\n",
    "    clf_knn = clf_knn_reg_grid.best_estimator_\n",
    "    return clf_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dummy classifiers creation:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TotallyDumbClassifier():\n",
    "    def fit(self,_,y_train):\n",
    "        self.most_common = max(set(y_train), key=y_train.count)\n",
    "    def predict(self,x_test):\n",
    "        return np.array([self.most_common]*x_test.shape[0])\n",
    "    def __str__(self):\n",
    "        \n",
    "        return \"TotallyDumbClassifier(most_common='%d')\" % self.most_common\n",
    "        \n",
    "def get_dummy_classifier(X_train,y_train):\n",
    "    clf_dummy = DummyClassifier(random_state=42)\n",
    "    clf_dummy.fit(X_train, y_train) \n",
    "    return clf_dummy\n",
    "\n",
    "def get_dummy_custom_classifier(X_train,y_train):\n",
    "    clf_dummy_custom = TotallyDumbClassifier()\n",
    "    clf_dummy_custom.fit(X_train, y_train)\n",
    "    return clf_dummy_custom   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classifiers evaluation:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stat_latex():\n",
    "    lat = ''\n",
    "     \n",
    "    return lat\n",
    "\n",
    "def statistics(predict,y_test):\n",
    "    print 'Number of predictions per category:'\n",
    "    counter = {}\n",
    "    for p in annotation_classes:\n",
    "        counter[p] = 0\n",
    "        \n",
    "    for p in predict:\n",
    "        counter[p] += 1\n",
    "    for p in annotation_classes:\n",
    "        print p,'-', counter[p]\n",
    "    b = 0\n",
    "    for i in range(len(predict)):\n",
    "        if predict[i] == y_test[i]:\n",
    "            b += 1\n",
    "    print ''\n",
    "    print 'Confusion matrix:'\n",
    "    cm = confusion_matrix(y_test, predict)\n",
    " \n",
    "    print pandas.DataFrame.from_items([(tweet_categories[annotation_classes[i]],cm[i]) for i in range(len(annotation_classes))],orient='index', columns=annotation_classes)\n",
    "    print ''\n",
    "    precisions, recalls, F1s, supports = precision_recall_fscore_support(y_test, predict)\n",
    "    matrix_items = [(tweet_categories[annotation_classes[i]],[precisions[i],recalls[i],F1s[i]]) for i in range(len(annotation_classes))]\n",
    "    matrix_items.append(('Σ',precision_recall_fscore_support(y_test, predict, average='macro')[:-1]))\n",
    "    \n",
    "    print matrix_items\n",
    "    print pandas.DataFrame.from_items(matrix_items,orient='index', columns=['Precision','Recall','F1'])   \n",
    "\n",
    "    print ''\n",
    "    print \"Accuracy:  %f %s\" % ( accuracy_score(y_test, predict)*100,'%')\n",
    "    \n",
    "def evaluate_classifiers(clfs, X_test, y_test, X_all = None, y_all = None, cross_evaluation = False, cv = None):\n",
    "    for i in range(len(clfs)):\n",
    "        print clf_names[i]+'\\n'\n",
    "        clf = clfs[i]\n",
    "        print clf\n",
    "\n",
    "        pred = clf.predict(X_test)\n",
    "        statistics(pred,y_test)\n",
    "        if clf != clf_dummy_custom:\n",
    "            if cross_evaluation:\n",
    "                print 'Calculating cross-validation (cv = %d)...' % cv\n",
    "                scores = cross_val_score(clf, X_all, y_all, cv=cv,scoring='f1_weighted')\n",
    "                print scores\n",
    "                print(\"Cross-validated accuracy: %0.2f (+/- %0.2f)\" % (scores.mean()*100, scores.std() * 2))\n",
    "\n",
    "        print '-'*70+'\\n'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>All used classifiers:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for optimal SVM classifier...\n",
      "Parameters search: {'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9]}\n",
      "Searching for optimal LogReg classifier...\n",
      "Parameters search: {'C': [0.25, 2.75, 5.25, 7.75, 10.25, 12.75, 15.25, 17.75, 20.25, 22.75, 25.25, 27.75, 30.25, 32.75, 35.25, 37.75, 40.25, 42.75]}\n",
      "Searching for optimal KNN classifier...\n",
      "Parameters search: {'n_neighbors': [1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49]}\n",
      "DummyClassifier(constant=None, random_state=42, strategy='stratified')\n",
      "\n",
      "TotallyDumbClassifier(most_common='7')\n",
      "\n",
      "LinearSVC(C=0.5, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "\n",
      "LogisticRegression(C=15.25, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0)\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_neighbors=28, p=2, weights='uniform')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_dummy = get_dummy_classifier(X_train,y_train)\n",
    "clf_dummy_custom = get_dummy_custom_classifier(X_train,y_train)\n",
    "if presentation_mode:\n",
    "    clf_svm = pickle.load( open( \"saved_objects/clf_svm.pkl\", \"rb\" ) )\n",
    "    clf_log_reg = pickle.load( open( \"saved_objects/clf_log_reg.pkl\", \"rb\" ) )\n",
    "    clf_knn = pickle.load( open( \"saved_objects/clf_knn.pkl\", \"rb\" ) )\n",
    "else:\n",
    "    clf_svm = get_svm_classifier(X_train,y_train)\n",
    "    pickle.dump( clf_svm, open( \"saved_objects/clf_svm.pkl\", \"wb\" ) )\n",
    "    \n",
    "    clf_log_reg = get_logReg_classifier(X_train,y_train)\n",
    "    pickle.dump( clf_log_reg, open( \"saved_objects/clf_log_reg.pkl\", \"wb\" ) )\n",
    "    \n",
    "    clf_knn = get_knn_classifier(X_train,y_train)\n",
    "    pickle.dump( clf_knn, open( \"saved_objects/clf_knn.pkl\", \"wb\" ) )\n",
    "    \n",
    "    \n",
    "clfs = [clf_dummy,clf_dummy_custom,clf_svm,clf_log_reg,clf_knn]\n",
    "\n",
    "for clf in clfs:\n",
    "    print clf\n",
    "    print ''\n",
    "\n",
    "clf_names = ['Dummy classifier','Totally dumb classifier (returns most common)','Support vector machine','Logistic regression','k-nearest neighbors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier\n",
      "\n",
      "DummyClassifier(constant=None, random_state=42, strategy='stratified')\n",
      "Number of predictions per category:\n",
      "2 - 277\n",
      "3 - 332\n",
      "4 - 462\n",
      "5 - 294\n",
      "6 - 798\n",
      "7 - 1054\n",
      "\n",
      "Confusion matrix:\n",
      "                  2   3    4   5    6    7\n",
      "DEALS            28  34   41  32   66  101\n",
      "NEWS_TECHNOLOGY  32  46   39  25   79  105\n",
      "NEWS_POLITICS    46  61   77  54  119  169\n",
      "NEWS_SPORT       27  23   43  23   85  112\n",
      "NEWS_REST        66  81  104  72  221  264\n",
      "REST             78  87  158  88  228  303\n",
      "\n",
      "[('DEALS', [0.10108303249097472, 0.092715231788079472, 0.096718480138169263]), ('NEWS_TECHNOLOGY', [0.13855421686746988, 0.1411042944785276, 0.1398176291793313]), ('NEWS_POLITICS', [0.16666666666666666, 0.14638783269961977, 0.15587044534412955]), ('NEWS_SPORT', [0.078231292517006806, 0.073482428115015971, 0.075782537067545314]), ('NEWS_REST', [0.27694235588972432, 0.27351485148514854, 0.27521793275217937]), ('REST', [0.28747628083491461, 0.321656050955414, 0.30360721442885774]), ('\\xce\\xa3', (0.17482564087779284, 0.17481011492030088, 0.17450237315170211))]\n",
      "                 Precision    Recall        F1\n",
      "DEALS             0.101083  0.092715  0.096718\n",
      "NEWS_TECHNOLOGY   0.138554  0.141104  0.139818\n",
      "NEWS_POLITICS     0.166667  0.146388  0.155870\n",
      "NEWS_SPORT        0.078231  0.073482  0.075783\n",
      "NEWS_REST         0.276942  0.273515  0.275218\n",
      "REST              0.287476  0.321656  0.303607\n",
      "Σ                 0.174826  0.174810  0.174502\n",
      "\n",
      "Accuracy:  21.697233 %\n",
      "Calculating cross-validation (cv = 10)...\n",
      "[ 0.1971858   0.21366212  0.21798287  0.2001308   0.21594581  0.19839653\n",
      "  0.20440961  0.23852446  0.22989124  0.19022231]\n",
      "Cross-validated accuracy: 21.06 (+/- 0.03)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Totally dumb classifier (returns most common)\n",
      "\n",
      "TotallyDumbClassifier(most_common='7')\n",
      "Number of predictions per category:\n",
      "2 - 0\n",
      "3 - 0\n",
      "4 - 0\n",
      "5 - 0\n",
      "6 - 0\n",
      "7 - 3217\n",
      "\n",
      "Confusion matrix:\n",
      "                 2  3  4  5  6    7\n",
      "DEALS            0  0  0  0  0  302\n",
      "NEWS_TECHNOLOGY  0  0  0  0  0  326\n",
      "NEWS_POLITICS    0  0  0  0  0  526\n",
      "NEWS_SPORT       0  0  0  0  0  313\n",
      "NEWS_REST        0  0  0  0  0  808\n",
      "REST             0  0  0  0  0  942\n",
      "\n",
      "[('DEALS', [0.0, 0.0, 0.0]), ('NEWS_TECHNOLOGY', [0.0, 0.0, 0.0]), ('NEWS_POLITICS', [0.0, 0.0, 0.0]), ('NEWS_SPORT', [0.0, 0.0, 0.0]), ('NEWS_REST', [0.0, 0.0, 0.0]), ('REST', [0.29281939695368353, 1.0, 0.45299350805482086]), ('\\xce\\xa3', (0.048803232825613919, 0.16666666666666666, 0.075498918009136814))]\n",
      "                 Precision    Recall        F1\n",
      "DEALS             0.000000  0.000000  0.000000\n",
      "NEWS_TECHNOLOGY   0.000000  0.000000  0.000000\n",
      "NEWS_POLITICS     0.000000  0.000000  0.000000\n",
      "NEWS_SPORT        0.000000  0.000000  0.000000\n",
      "NEWS_REST         0.000000  0.000000  0.000000\n",
      "REST              0.292819  1.000000  0.452994\n",
      "Σ                 0.048803  0.166667  0.075499\n",
      "\n",
      "Accuracy:  29.281940 %\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Support vector machine\n",
      "\n",
      "LinearSVC(C=0.5, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Number of predictions per category:\n",
      "2 - 280\n",
      "3 - 275\n",
      "4 - 507\n",
      "5 - 299\n",
      "6 - 847\n",
      "7 - 1009\n",
      "\n",
      "Confusion matrix:\n",
      "                   2    3    4    5    6    7\n",
      "DEALS            241    1    0    1   19   40\n",
      "NEWS_TECHNOLOGY    0  213    0    0   79   34\n",
      "NEWS_POLITICS      1    1  440    8   43   33\n",
      "NEWS_SPORT         0    0    4  267   23   19\n",
      "NEWS_REST         15   43   52   14  494  190\n",
      "REST              23   17   11    9  189  693\n",
      "\n",
      "[('DEALS', [0.86071428571428577, 0.79801324503311255, 0.82817869415807566]), ('NEWS_TECHNOLOGY', [0.77454545454545454, 0.65337423312883436, 0.70881863560732128]), ('NEWS_POLITICS', [0.86785009861932938, 0.83650190114068446, 0.85188770571151984]), ('NEWS_SPORT', [0.8929765886287625, 0.85303514376996803, 0.87254901960784315]), ('NEWS_REST', [0.5832349468713105, 0.61138613861386137, 0.59697885196374623]), ('REST', [0.68681863230921703, 0.73566878980891715, 0.7104049205535623]), ('\\xce\\xa3', (0.77769000111472664, 0.74799657524922969, 0.76146963793367795))]\n",
      "                 Precision    Recall        F1\n",
      "DEALS             0.860714  0.798013  0.828179\n",
      "NEWS_TECHNOLOGY   0.774545  0.653374  0.708819\n",
      "NEWS_POLITICS     0.867850  0.836502  0.851888\n",
      "NEWS_SPORT        0.892977  0.853035  0.872549\n",
      "NEWS_REST         0.583235  0.611386  0.596979\n",
      "REST              0.686819  0.735669  0.710405\n",
      "Σ                 0.777690  0.747997  0.761470\n",
      "\n",
      "Accuracy:  72.987255 %\n",
      "Calculating cross-validation (cv = 10)...\n",
      "[ 0.72857889  0.69744372  0.73292132  0.74928247  0.71510212  0.70343401\n",
      "  0.69717318  0.71759453  0.70996088  0.71490318]\n",
      "Cross-validated accuracy: 71.66 (+/- 0.03)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Logistic regression\n",
      "\n",
      "LogisticRegression(C=15.25, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0)\n",
      "Number of predictions per category:\n",
      "2 - 279\n",
      "3 - 267\n",
      "4 - 508\n",
      "5 - 284\n",
      "6 - 857\n",
      "7 - 1022\n",
      "\n",
      "Confusion matrix:\n",
      "                   2    3    4    5    6    7\n",
      "DEALS            240    0    0    1   22   39\n",
      "NEWS_TECHNOLOGY    0  205    0    0   90   31\n",
      "NEWS_POLITICS      1    1  439    7   46   32\n",
      "NEWS_SPORT         0    0    4  259   25   25\n",
      "NEWS_REST         15   45   54   10  484  200\n",
      "REST              23   16   11    7  190  695\n",
      "\n",
      "[('DEALS', [0.86021505376344087, 0.79470198675496684, 0.82616179001721168]), ('NEWS_TECHNOLOGY', [0.76779026217228463, 0.62883435582822089, 0.69139966273187181]), ('NEWS_POLITICS', [0.86417322834645671, 0.83460076045627374, 0.84912959381044484]), ('NEWS_SPORT', [0.9119718309859155, 0.82747603833865813, 0.86767169179229475]), ('NEWS_REST', [0.56476079346557762, 0.59900990099009899, 0.58138138138138151]), ('REST', [0.68003913894324852, 0.73779193205944793, 0.70773930753564152]), ('\\xce\\xa3', (0.77482505127948731, 0.73706916240461107, 0.75391390454480767))]\n",
      "                 Precision    Recall        F1\n",
      "DEALS             0.860215  0.794702  0.826162\n",
      "NEWS_TECHNOLOGY   0.767790  0.628834  0.691400\n",
      "NEWS_POLITICS     0.864173  0.834601  0.849130\n",
      "NEWS_SPORT        0.911972  0.827476  0.867672\n",
      "NEWS_REST         0.564761  0.599010  0.581381\n",
      "REST              0.680039  0.737792  0.707739\n",
      "Σ                 0.774825  0.737069  0.753914\n",
      "\n",
      "Accuracy:  72.179049 %\n",
      "Calculating cross-validation (cv = 10)...\n",
      "[ 0.71484217  0.69911671  0.72023113  0.74301181  0.70375958  0.70052702\n",
      "  0.69863422  0.72177614  0.70898412  0.70699828]\n",
      "Cross-validated accuracy: 71.18 (+/- 0.03)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "k-nearest neighbors\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_neighbors=28, p=2, weights='uniform')\n",
      "Number of predictions per category:\n",
      "2 - 286\n",
      "3 - 227\n",
      "4 - 439\n",
      "5 - 285\n",
      "6 - 478\n",
      "7 - 1502\n",
      "\n",
      "Confusion matrix:\n",
      "                   2    3    4    5    6    7\n",
      "DEALS            224    0    1    1    7   69\n",
      "NEWS_TECHNOLOGY    1  180    4    4   42   95\n",
      "NEWS_POLITICS      9    0  361    8   22  126\n",
      "NEWS_SPORT         0    1    5  246    9   52\n",
      "NEWS_REST         14   34   54   16  322  368\n",
      "REST              38   12   14   10   76  792\n",
      "\n",
      "[('DEALS', [0.78321678321678323, 0.74172185430463577, 0.76190476190476197]), ('NEWS_TECHNOLOGY', [0.79295154185022021, 0.55214723926380371, 0.65099457504520797]), ('NEWS_POLITICS', [0.82232346241457854, 0.68631178707224338, 0.7481865284974093]), ('NEWS_SPORT', [0.86315789473684212, 0.78594249201277955, 0.82274247491638797]), ('NEWS_REST', [0.67364016736401677, 0.39851485148514854, 0.5007776049766719]), ('REST', [0.52729693741677763, 0.84076433121019112, 0.64811783960720137]), ('\\xce\\xa3', (0.74376446449986977, 0.66756709255813362, 0.68878729749127343))]\n",
      "                 Precision    Recall        F1\n",
      "DEALS             0.783217  0.741722  0.761905\n",
      "NEWS_TECHNOLOGY   0.792952  0.552147  0.650995\n",
      "NEWS_POLITICS     0.822323  0.686312  0.748187\n",
      "NEWS_SPORT        0.863158  0.785942  0.822742\n",
      "NEWS_REST         0.673640  0.398515  0.500778\n",
      "REST              0.527297  0.840764  0.648118\n",
      "Σ                 0.743764  0.667567  0.688787\n",
      "\n",
      "Accuracy:  66.055331 %\n",
      "Calculating cross-validation (cv = 10)...\n",
      "[ 0.6841768   0.64053976  0.67626825  0.67261312  0.65504173  0.63019962\n",
      "  0.65321677  0.63019868  0.64250057  0.63566909]\n",
      "Cross-validated accuracy: 65.20 (+/- 0.04)\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifiers(clfs, X_test, y_test, X_train, y_train, cross_evaluation = True, cv = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing and classification of tweet text:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def purge(tweet_text):\n",
    "    purged = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', tweet_text).strip()\n",
    "    purged_spl = purged.split(' ')\n",
    "    if (purged) and (purged_spl[-1][0] in '^'):\n",
    "        purged = ' '.join(purged_spl[:-1]).strip()\n",
    "    for forbidden in '!&.?-–^,;:\"\\'/[]{}()<>':\n",
    "        while forbidden in purged:\n",
    "            purged = purged.replace(forbidden,'')\n",
    "    \n",
    "    purged = purged.strip()\n",
    "    if purged == '':\n",
    "        return 'poveznica'\n",
    "    return purged\n",
    "\n",
    "def classify_tweet_text(tweet_text, clfs, vectorizer):\n",
    "    target_name = intermediate_dir + 'purged_tweet.txt'\n",
    "    \n",
    "    open(target_name,'w').write(purge(tweet_text)+'\\n')\n",
    "    command = ['\"NLPToolkit.exe\"','-pre', '-pos', '-lm', '\"'+target_name+'\"', '\"'+target_name.replace('purged','pre_purged')+'\"']\n",
    "    #print ' '.join(command)\n",
    "    call(' '.join(command),shell=True)\n",
    "    f = open(target_name.replace('purged','pre_purged'),'r')\n",
    "    processed_tweet_text = ''\n",
    "    for line in f:\n",
    "        if ( line != \"\\n\"):\n",
    "            word = line.split('\\t')[2].strip().lower()\n",
    "            processed_tweet_text += word+' '\n",
    "        else:\n",
    "            processed_tweet_text = processed_tweet_text.rstrip()\n",
    "    f.close()\n",
    "    os.remove(target_name)\n",
    "    os.remove(target_name.replace('purged','pre_purged'))\n",
    "    processed_tweet_text = processed_tweet_text\n",
    "    print 'Original text: \"%s\"' % tweet_text\n",
    "    print 'Preprocessed text: \"%s\"' % processed_tweet_text\n",
    "    print ''\n",
    "    X_tweet = enhance_vectors([processed_tweet_text],vectorizer.transform([processed_tweet_text]), print_progress = False)\n",
    "    \n",
    "    for i in range(len(clfs)):\n",
    "        print clf_names[i],'-',\n",
    "        clf = clfs[i]\n",
    "        print tweet_categories[clf.predict(X_tweet)[0]]\n",
    "        print ''    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Whole dataset as train set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyClassifier(constant=None, random_state=42, strategy='stratified')\n",
      "\n",
      "TotallyDumbClassifier(most_common='7')\n",
      "\n",
      "LinearSVC(C=0.9, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "\n",
      "LogisticRegression(C=40.25, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0)\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_neighbors=40, p=2, weights='uniform')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_dummy_all = get_dummy_classifier(X_all,y_all)\n",
    "clf_dummy_custom_all = get_dummy_custom_classifier(X_all,y_all)\n",
    "    \n",
    "    \n",
    "if presentation_mode:\n",
    "    clf_svm_all = pickle.load( open( \"saved_objects/clf_svm_all.pkl\", \"rb\" ) )\n",
    "    clf_log_reg_all = pickle.load( open( \"saved_objects/clf_log_reg_all.pkl\", \"rb\" ) )\n",
    "    clf_knn_all = pickle.load( open( \"saved_objects/clf_knn_all.pkl\", \"rb\" ) )\n",
    "else:\n",
    "    clf_svm_all = get_svm_classifier(X_all,y_all)\n",
    "    pickle.dump( clf_svm_all, open( \"saved_objects/clf_svm_all.pkl\", \"wb\" ) )\n",
    "    \n",
    "    clf_log_reg_all = get_logReg_classifier(X_all,y_all)\n",
    "    pickle.dump( clf_log_reg_all, open( \"saved_objects/clf_log_reg_all.pkl\", \"wb\" ) )\n",
    "    \n",
    "    clf_knn_all = get_knn_classifier(X_all,y_all)\n",
    "    pickle.dump( clf_knn_all, open( \"saved_objects/clf_knn_all.pkl\", \"wb\" ) )\n",
    "clfs_all = [clf_dummy_all,clf_dummy_custom_all,clf_svm_all,clf_log_reg_all,clf_knn_all]\n",
    "\n",
    "for clf in clfs_all:\n",
    "    print clf\n",
    "    print ''\n",
    "\n",
    "clf_names_all = ['Dummy classifier','Totally dumb classifier (returns most common)','Support vector machine','Logistic regression','k-nearest neighbors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Single tweet classification:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \"Karamarko je rekao da je Petrov lagao\"\n",
      "Preprocessed text: \"﻿karamarko biti reći da biti petrov lagati\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_POLITICS\n",
      "\n",
      "Logistic regression - NEWS_POLITICS\n",
      "\n",
      "k-nearest neighbors - NEWS_POLITICS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = 'Karamarko je rekao da je Petrov lagao'\n",
    "classify_tweet_text(example,clfs_all,vectorizer_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Multiple tweets (file) classification:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \"Hrvatska je kažnjena s dvije utakmice bez navijača u  kvalifikacijama za SP, @UvijekVjerniMan reagirali su na odluku.\n",
      "\"\n",
      "Preprocessed text: \"﻿hrvatska biti kažnjen s dva utakmica bez navijač u kvalifikacija za sp @ uvijekvjerniman reagirati biti na odluka\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_SPORT\n",
      "\n",
      "Logistic regression - NEWS_SPORT\n",
      "\n",
      "k-nearest neighbors - NEWS_SPORT\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Damir Burić više nije trener @hajduk! Novi trener je Slovenac Pušnik ili Željko Kopić. > http://bit.ly/1PkMsMb \n",
      "\"\n",
      "Preprocessed text: \"﻿damir burić vio biti trener @ hajduk nov trener biti slovenac pušnik ili željko kopić\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_SPORT\n",
      "\n",
      "Logistic regression - NEWS_SPORT\n",
      "\n",
      "k-nearest neighbors - NEWS_SPORT\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"MOO obavijestio je sve saveze o mjerama koje će poduzeti kako bi potvrdio svoju nultu toleranciju na doping\n",
      "\"\n",
      "Preprocessed text: \"﻿moo obavijestiti biti sav savez o mjera koji htjeti poduzeti kako biti potvrditi svoj nulti tolerancija na doping\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_SPORT\n",
      "\n",
      "Logistic regression - NEWS_SPORT\n",
      "\n",
      "k-nearest neighbors - NEWS_SPORT\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Doigravanje za prvaka Hrvatske: @KKCedevita povela protiv #KKCibona http://bit.ly/1U5RgRY \n",
      "\"\n",
      "Preprocessed text: \"﻿doigravanje za prvak hrvatska @ kkcedevita povesti protiv # kkcibon\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_SPORT\n",
      "\n",
      "Logistic regression - NEWS_SPORT\n",
      "\n",
      "k-nearest neighbors - NEWS_SPORT\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Veslanje, Svjetski kup: #LukaRadonic pobijedio u Luzernu http://bit.ly/1shY5t5 \n",
      "\"\n",
      "Preprocessed text: \"﻿veslanjati svjetski kup # lukaradonic pobijediti u luzerna\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_SPORT\n",
      "\n",
      "Logistic regression - NEWS_SPORT\n",
      "\n",
      "k-nearest neighbors - NEWS_SPORT\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Vatreni u Međimurju završili prvi dio priprema za @UEFAEURO, izbornik Čačić zadovoljan http://bit.ly/1sRTpuI\n",
      "\"\n",
      "Preprocessed text: \"﻿vatren u međimurj završiti prvi dio priprema za @ uefaeuro izbornik čačić zadovoljan\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_SPORT\n",
      "\n",
      "Logistic regression - NEWS_SPORT\n",
      "\n",
      "k-nearest neighbors - NEWS_SPORT\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Na poluvremenu treće polufinalne utakmice PH @kk_cibona vodi protiv @kkzadar  43.27.\n",
      "\"\n",
      "Preprocessed text: \"﻿n poluvrijeme treći polufinalan utakmica p @ kk _ cibona voditi protiv @ kkzadar 4327\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_SPORT\n",
      "\n",
      "Logistic regression - NEWS_SPORT\n",
      "\n",
      "k-nearest neighbors - NEWS_SPORT\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"GONG: Pozivamo na 'samoraspuštanje' Sabora i raspisivanje izbore najkasnije do 17. srpnja\n",
      "\"\n",
      "Preprocessed text: \"﻿gong pozivati na samoraspuštanje sabor i raspisivanje izbor najkasnija do 17 srpanj\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_POLITICS\n",
      "\n",
      "Logistic regression - NEWS_POLITICS\n",
      "\n",
      "k-nearest neighbors - NEWS_POLITICS\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"'Orešković je zabio zadnji čavao u lijes Vladi'\n",
      "\"\n",
      "Preprocessed text: \"﻿orešković biti zabiti zadnji čavao u lijes vlade\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_POLITICS\n",
      "\n",
      "Logistic regression - NEWS_POLITICS\n",
      "\n",
      "k-nearest neighbors - NEWS_POLITICS\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"[FOTO] Cvitanović: Ovo je sramota, ovo nije Dinamo!\n",
      "\"\n",
      "Preprocessed text: \"﻿foto cvitanović ovaj biti sramota ovaj biti dinamo\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_SPORT\n",
      "\n",
      "Logistic regression - NEWS_SPORT\n",
      "\n",
      "k-nearest neighbors - REST\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Centar pooštrio kazne za prepisivanje na ispitima i moguću krađu testova\n",
      "\"\n",
      "Preprocessed text: \"﻿centar pooštriti kazna za prepisivanje na ispit i mogući krađa test\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_REST\n",
      "\n",
      "Logistic regression - NEWS_REST\n",
      "\n",
      "k-nearest neighbors - NEWS_REST\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Jokić zahvaljuje i bebama i mladima i bakama, Šustar poručuje da ne pomišlja na ostavku\n",
      "\"\n",
      "Preprocessed text: \"﻿jokić zahvaljuja i beba i mlad i bakama šustar poručivati da ne pomišljati na ostavka\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_REST\n",
      "\n",
      "Logistic regression - NEWS_REST\n",
      "\n",
      "k-nearest neighbors - NEWS_POLITICS\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Udruga Franak zahtijeva prekršajne postupke protiv banaka\n",
      "\"\n",
      "Preprocessed text: \"﻿udruga franak zahtijevati prekršajan postupak protiv banka\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - REST\n",
      "\n",
      "Logistic regression - REST\n",
      "\n",
      "k-nearest neighbors - NEWS_POLITICS\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"HNS će glasati za opoziv Karamarka, traže raspisivanje parlamentarnih izbora\n",
      "\"\n",
      "Preprocessed text: \"﻿hns htjeti glasati za opoziv karamarko tražiti raspisivanje parlamentaran izbor\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_POLITICS\n",
      "\n",
      "Logistic regression - NEWS_POLITICS\n",
      "\n",
      "k-nearest neighbors - NEWS_POLITICS\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Povećati udio hrvatskih proizvoda u sustavu javne nabave\n",
      "\"\n",
      "Preprocessed text: \"﻿povećati udio hrvatski proizvod u sustav javan nabava\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_TECHNOLOGY\n",
      "\n",
      "Logistic regression - NEWS_REST\n",
      "\n",
      "k-nearest neighbors - REST\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Đukanović: Dubrovnik iz 1991. je sramota za Crnu Goru\n",
      "\"\n",
      "Preprocessed text: \"﻿đukanović dubrovnik iz 1991 biti sramota za crna gora\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - REST\n",
      "\n",
      "Logistic regression - REST\n",
      "\n",
      "k-nearest neighbors - REST\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Laku noć želi vam Index. http://fb.me/4bmdoSaCD \n",
      "\"\n",
      "Preprocessed text: \"﻿laku noć željeti vi index\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - REST\n",
      "\n",
      "Logistic regression - REST\n",
      "\n",
      "k-nearest neighbors - REST\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Petrov će se za 10ak minuta obraća javnosti!\n",
      "\"\n",
      "Preprocessed text: \"﻿petrov htjeti sebe za 10ak minuta obraćati javnost\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_POLITICS\n",
      "\n",
      "Logistic regression - NEWS_POLITICS\n",
      "\n",
      "k-nearest neighbors - NEWS_POLITICS\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Mladi Hasanbegović u društvu poznatih simpatizera kvinsliškog fašističkog režima.\n",
      "\"\n",
      "Preprocessed text: \"﻿mladi hasanbegović u društvo poznat simpatizera kvinsliški fašistički režim\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_POLITICS\n",
      "\n",
      "Logistic regression - NEWS_POLITICS\n",
      "\n",
      "k-nearest neighbors - NEWS_POLITICS\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Prodali prvu tvrtku s popisa.\n",
      "\"\n",
      "Preprocessed text: \"﻿prodati prvi tvrtka s popis\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - REST\n",
      "\n",
      "Logistic regression - REST\n",
      "\n",
      "k-nearest neighbors - REST\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"U trenutku smrti imao je samo 50 kilograma.\n",
      "\"\n",
      "Preprocessed text: \"﻿u trenutak smrt imati biti samo 50 kilogram\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_REST\n",
      "\n",
      "Logistic regression - NEWS_REST\n",
      "\n",
      "k-nearest neighbors - REST\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Hoće li napokon komentirati i činjenicu da se Vlada raspada? PRATITE NA INDEXU!\n",
      "\"\n",
      "Preprocessed text: \"﻿htjeti li napokon komentirati i činjenica da sebe vlada raspad pratita na indexu\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_POLITICS\n",
      "\n",
      "Logistic regression - NEWS_POLITICS\n",
      "\n",
      "k-nearest neighbors - REST\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Dokumenti objavljeni usred sukoba Petrova i Karamarka.\n",
      "\"\n",
      "Preprocessed text: \"﻿dokument objavljen usred sukob petrova i karamarka\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_POLITICS\n",
      "\n",
      "Logistic regression - NEWS_POLITICS\n",
      "\n",
      "k-nearest neighbors - NEWS_POLITICS\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Nakon godinu dana javnog ispitivanja službeno je dostupna nova verzija Microsoftovog SQL servera: https://is.gd/7tlDkp  ^kb\n",
      "\"\n",
      "Preprocessed text: \"﻿nakon godina dan javan ispitivanje službeno biti dostupan nov verzija microsoftov sql server\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_TECHNOLOGY\n",
      "\n",
      "Logistic regression - NEWS_TECHNOLOGY\n",
      "\n",
      "k-nearest neighbors - REST\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Amerikanci najviše vole Samsungov Galaxy Note 5: https://is.gd/6Ct89F  ^mp\n",
      "\"\n",
      "Preprocessed text: \"﻿amerikanak mnogo voljeti samsungov galaxy nota 5\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_TECHNOLOGY\n",
      "\n",
      "Logistic regression - NEWS_TECHNOLOGY\n",
      "\n",
      "k-nearest neighbors - REST\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Europska komisija želi zaštititi Uber, Airbnb i slične kompanije: https://is.gd/3B4No6  ^sv\n",
      "\"\n",
      "Preprocessed text: \"﻿europski komisija željeti zaštititi uber airbnb i sličan kompanija\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_REST\n",
      "\n",
      "Logistic regression - NEWS_REST\n",
      "\n",
      "k-nearest neighbors - NEWS_TECHNOLOGY\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Samsung poziva svoje korisnike da ne instaliraju Windows 10 : https://is.gd/6JIECO  ^mp\n",
      "\"\n",
      "Preprocessed text: \"﻿samsung pozivati svoj korisnik da ne instalirati windows 10\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_TECHNOLOGY\n",
      "\n",
      "Logistic regression - NEWS_TECHNOLOGY\n",
      "\n",
      "k-nearest neighbors - NEWS_TECHNOLOGY\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Original text: \"Intel na Computexu 2016: pet najvažnijih informacija: https://is.gd/zjW5wO  ^sv\n",
      "\"\n",
      "Preprocessed text: \"﻿intel na computex 2016 pet važan informacija\"\n",
      "\n",
      "Dummy classifier - NEWS_TECHNOLOGY\n",
      "\n",
      "Totally dumb classifier (returns most common) - REST\n",
      "\n",
      "Support vector machine - NEWS_TECHNOLOGY\n",
      "\n",
      "Logistic regression - NEWS_TECHNOLOGY\n",
      "\n",
      "k-nearest neighbors - NEWS_TECHNOLOGY\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file_example in open('examples.txt','r'):\n",
    "    classify_tweet_text(file_example,clfs_all,vectorizer_all)\n",
    "    print '-'*70+'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Classifier objects creation:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs_dict = {}\n",
    "clfs_dict['clf_svm'] = clf_svm_all\n",
    "clfs_dict['clf_log_reg'] = clf_log_reg_all\n",
    "clfs_dict['clf_knn'] = clf_knn_all\n",
    "\n",
    "pickle.dump( clfs_dict, open( \"saved_objects/classifiers.pkl\", \"wb\" ) )\n",
    "pickle.dump( vectorizer_all, open( \"saved_objects/vectorizer.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs_dict = pickle.load( open( \"saved_objects/classifiers.pkl\", \"rb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
