{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of Zagreb<br>\n",
    "Faculty of Electrical Engineering and Computing\n",
    "\n",
    "# Text Analysis and Retrieval (TAR)\n",
    "\n",
    "<a href=\"http://www.fer.unizg.hr/predmet/apt\">http://www.fer.unizg.hr/predmet/apt</a>\n",
    "\n",
    "2015./2016.\n",
    "\n",
    "# Project theme 13: Tweet classification\n",
    "\n",
    "\n",
    "(c) 2016 Group nedovrs: Tomislav Marinković, Josip Milić, Domagoj Pereglin\n",
    "\n",
    "*Version 0.95*\n",
    "\n",
    "Date: **05.06.2016.**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tweet preprocessor</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Packages:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from subprocess import call, Popen\n",
    "import os\n",
    "import codecs\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tweet text cleaner:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def purge(str):\n",
    "    purged =re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', str).strip()\n",
    "    purged_spl = purged.split(' ')\n",
    "    if (purged) and (purged_spl[-1][0] in '^'):\n",
    "        purged = ' '.join(purged_spl[:-1]).strip()\n",
    "    for forbidden in '!&.?-–^,;:\"\\'/[]{}()<>':\n",
    "        while forbidden in purged:\n",
    "            purged = purged.replace(forbidden,'')\n",
    "    \n",
    "    purged = purged.strip()\n",
    "    return purged\n",
    "\n",
    "def get_tweet( annotation_test ):\n",
    "    dataset_dir_path = '../datasets'\n",
    "    dataset_test,tweet_id_test,tweet_category_test = annotation_test.split(';')\n",
    "    tweet_category_test = int(tweet_category_test)\n",
    "    dataset_file_test = open(dataset_dir_path + '\\\\' + dataset_test + '_tweets.txt','r')\n",
    "    for line in dataset_file_test:\n",
    "        if tweet_id_test == line.split('|',2)[0]:\n",
    "            pur = purge(line.rstrip().split('|',2)[2])\n",
    "            if pur == '':\n",
    "                return 'poveznica'\n",
    "            return pur\n",
    "    dataset_file_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Annotated dataset preprocessing:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset bugonline...\n",
      "Processing dataset hrtsport...\n",
      "Processing dataset indexhr...\n",
      "Processing dataset oglasnik...\n",
      "Processing dataset politikaplus...\n",
      "Processing dataset posaohr...\n",
      "Processing finished!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def important ( str ):\n",
    "    response = True\n",
    "    for word in stopWords:\n",
    "        if ( word == str ):\n",
    "            response = False;    \n",
    "    return response\n",
    "\n",
    "annotations_dir = '../annotations/'\n",
    "annotations_file_names = os.listdir(annotations_dir)\n",
    "dataset_names = [x.split('_')[1] for x in annotations_file_names]\n",
    "classes = {}\n",
    "for dataset_name in dataset_names:\n",
    "    target_name = \"intermediate/purged_%s_tweets.txt\" % dataset_name\n",
    "    print 'Processing dataset %s...' % dataset_name\n",
    "    target = open(target_name, 'w')\n",
    "    classes[dataset_name] = []\n",
    "    annotations_file = open('../annotations/annotations_%s_tweets.csv' % dataset_name,'r')\n",
    "    for line in annotations_file:\n",
    "        target.write(get_tweet(line) + '\\n')\n",
    "        classes[dataset_name].append(line.rstrip().split(';')[1:])\n",
    "    target.close()\n",
    "    command = ['\"NLPToolkit.exe\"','-pre', '-pos', '-lm', '\"'+target_name+'\"', '\"'+target_name.replace('purged','pre_purged')+'\"']\n",
    "    #print ' '.join(command)\n",
    "    call(' '.join(command),shell=True)\n",
    "    \n",
    "    target = open('processed/processed_%s_tweets.txt' % dataset_name, 'w')\n",
    "    f = open('intermediate/pre_purged_%s_tweets.txt' % dataset_name,'r')\n",
    "    counter = 0;\n",
    "    for line in f:\n",
    "        if ( line != \"\\n\"):\n",
    "            word = line.split('\\t')[2].strip().lower()\n",
    "            #if important(word):\n",
    "            target.write(word+' ')\n",
    "        else:\n",
    "            #target.write('<|>'+classes[dataset_name][counter][0]+'<|>'+classes[dataset_name][counter][1]+'\\n')\n",
    "            target.write('<|>'+classes[dataset_name][counter][1]+'\\n')\n",
    "            counter = counter + 1\n",
    "    f.close()\n",
    "    target.close()\n",
    "print 'Processing finished!'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
